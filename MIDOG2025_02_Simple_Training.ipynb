{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDOG 2025 Dataset Preparation and Alrogithm Setup\n",
    "\n",
    "This notebook will show you how you could setup the MIDOG++ dataset and train a simple detection method. Before starting with this notebook, you should have a look at the `MIDOG2025_01_Exploratory_Analysis.ipynb` to get familiar with the data. If you have not yet downloaded the MIDOG++ dataset, check out the previous notebook or download the dataset with the `download_MIDOGpp.py` script.\n",
    "\n",
    "Here, we will go through the following steps:\n",
    "1. Prepare the MIDOG++ dataset for algorithm development.\n",
    "2. Setup a simple object detection pipeline.\n",
    "3. Evaluate the model on large images. \n",
    "\n",
    "**Note: This is notebook should just give you an idea of how to approach the challenge. You can be creative and set the dataset up differently. You are also encouraged to use different models. Have a look at the methods from previous challenges get a better picture of the task. Here is a link to the [MIDOG 2022 Overview Paper](https://www.sciencedirect.com/science/article/pii/S136184152400080X).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Make sure that you set up your environment correctly and downloaded the MIDOG++ dataset by following the instructions of the `README.md` or from the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import openslide \n",
    "import matplotlib.pyplot as plt \n",
    "import json \n",
    "import plotly.express as px \n",
    "import torch \n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the MIDOG++ dataset for algorithm development\n",
    "\n",
    "In the following steps, we will split the data into a training, validation and test split to get started with the development of our detection algorithm. For easier handling and visualization we will convert the `json` database file into a `pandas` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_datasets(\n",
    "        json_file, \n",
    "        train_ratio: float = 0.7, \n",
    "        val_ratio: float = 0.2, \n",
    "        test_ratio: float = 0.1,\n",
    "        random_seed: int = 42\n",
    "        ):\n",
    "    \"\"\"Converts the json file to pandas dataframe and creates train, val, and test split containing all tumortypes.\"\"\"\n",
    "\n",
    "    # Verify ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10, 'Ratios must sum to 1.'\n",
    "\n",
    "    database = json.load(open(json_file, 'rb'))\n",
    "\n",
    "    # Read image data\n",
    "    image_df = pd.DataFrame.from_dict(database['images'])\n",
    "    image_df = image_df.drop(columns=['license', 'width', 'height'])\n",
    "    image_df = image_df.rename({'id':'image_id', 'tumor_type':'tumortype', 'file_name':'filename'}, axis=1)\n",
    "\n",
    "    # Group by tumortype and sample training split\n",
    "    train_ids = image_df.groupby('tumortype').sample(frac=train_ratio, random_state=random_seed)['image_id']\n",
    "\n",
    "    # Sample validation split from the remaining samples \n",
    "    remaining_df = image_df.query('image_id not in @train_ids')\n",
    "    adjusted_val_ratio = val_ratio / (val_ratio + test_ratio)\n",
    "    valid_ids = remaining_df.groupby('tumortype').sample(frac=adjusted_val_ratio, random_state=random_seed)['image_id']\n",
    "\n",
    "    # Assign splits, test samples are neither train nor val samples \n",
    "    image_df['split'] = 'test'  \n",
    "    image_df.loc[image_df['image_id'].isin(train_ids), 'split'] = 'train'\n",
    "    image_df.loc[image_df['image_id'].isin(valid_ids), 'split'] = 'val'\n",
    "\n",
    "    # Read annotations and convert to center locations \n",
    "    annotations_df = pd.DataFrame.from_dict(database['annotations'])\n",
    "    annotations_df = annotations_df.assign(x=annotations_df['bbox'].apply(lambda x: int((x[0] + x[2]) / 2)))\n",
    "    annotations_df = annotations_df.assign(y=annotations_df['bbox'].apply(lambda x: int((x[1] + x[3]) / 2)))\n",
    "    annotations_df = annotations_df.drop(columns=['bbox', 'labels', 'id'])\n",
    "\n",
    "    # Merge data, rename and rearrange\n",
    "    comb_df = image_df.merge(annotations_df, how='right', on='image_id')\n",
    "    comb_df = comb_df.rename({'category_id': 'label', 'image_id': 'slide'}, axis=1)\n",
    "    comb_df = comb_df[['x', 'y', 'label', 'filename', 'slide', 'split', 'tumortype']]\n",
    "\n",
    "    return comb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train, val, and test split\n",
    "\n",
    "For the purpose of this notebook, we will split the data into a 70/20/10 train, val, and test split. This is simply to show you how the pipeline works. For challenge participation you can think of different ways to split your data. You may want to train different models for tumortypes individually, or you want to use all images for training and validation and test your algorithm on the preliminary test set. The choice is up to you. However, it is always good practice to test your method on some unseen cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your dataset file \n",
    "dataset_file = '/data/patho/MIDOGpp/images/MIDOGpp.json'\n",
    "\n",
    "# Set your train, val, and test ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "random_seed = 42\n",
    "\n",
    "# Create the dataset\n",
    "dataset = create_train_val_test_datasets(dataset_file, train_ratio, val_ratio, test_ratio, random_seed)\n",
    "\n",
    "# Save the dataset \n",
    "dataset.to_csv('demo_dataset.csv', index=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the different splits that we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of files per train, val, test split\n",
    "print(dataset.groupby('split')['filename'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of mitotic figures in each split \n",
    "for split in dataset['split'].unique():\n",
    "    split_annos = dataset.query('split == @split')\n",
    "    row = []\n",
    "    for image_id in split_annos[\"slide\"].unique():\n",
    "        image_annos = split_annos.query('slide == @image_id')\n",
    "        row.append([image_id, len(image_annos[image_annos['label'] == 1]), \"mitotic figure\"])\n",
    "        row.append([image_id, len(image_annos[image_annos['label'] == 2]), \"hard negative\"])\n",
    "    tumortype_meta = pd.DataFrame(row, columns=[\"image_id\", \"total\", \"type\"])\n",
    "    fig = px.bar(tumortype_meta, x=\"image_id\", y=\"total\", color=\"type\", title=f\"{split}: Annotations per image\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have similar distribution of mitotic figures in each split. This helps our algorithm to handle cases with high and low mitotic figure density equally well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up simple object detection pipeline\n",
    "\n",
    "In the following steps we will create a relatively simple object detection pipeline. We use the `torchvision` library to create an `FCOS` detection algorithm. We use the `pytorch-lightning` library to train our model. \n",
    "\n",
    "You can also use the `train.py` script to train your own model. Here, we will follow the same steps as in the script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datamodule \n",
    "\n",
    "The dataloading pipeline in this repository is openslide-based and implements a class-specific online-sampling strategy. We sample patches around mitotic figures and hard negatives with a certain probability enabling us to sample a very diverse set of patches in every epoch. For instance, if we set `fg_prob=0.5` and `arb_prob=0.25`, 50% of the patches should contain at least one mitotic figure, 25% should contain at least on hard negative, and the other 25% are sampled completely at random. \n",
    "\n",
    "**Note: We use the hard negatives only to sample challenging patches for the model to learn decision boundaries more efficiently. We do not train the models to detect this class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datamodule import ObjectDetectionDataModule\n",
    "\n",
    "# Set the parameters\n",
    "img_dir = '/data/patho/MIDOGpp/images'\n",
    "dataset_file = 'demo_dataset.csv'\n",
    "domain_col = 'tumortype'\n",
    "box_format = 'cxcy'\n",
    "num_train_samples = 512\n",
    "num_val_samples = 256\n",
    "fg_prob = 0.5       # probability of patches with mitotic figures\n",
    "arb_prob = 0.25     # probability of random patches \n",
    "patch_size = 512\n",
    "batch_size = 12\n",
    "num_workers = 6\n",
    "\n",
    "# Create the datamodule\n",
    "dm = ObjectDetectionDataModule(\n",
    "    img_dir=img_dir,\n",
    "    dataset=dataset_file,\n",
    "    domain_col=domain_col, \n",
    "    box_format=box_format,\n",
    "    num_train_samples=num_train_samples,\n",
    "    num_val_samples=num_val_samples,\n",
    "    fg_prob=fg_prob,\n",
    "    arb_prob=arb_prob,\n",
    "    patch_size=patch_size,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_images(images, gt_boxes, gt_labels, pred_boxes=None, pred_labels=None, pred_scores=None, legend=False):\n",
    "    \"\"\"Visualized images with annotations and optional with predictions.\"\"\"\n",
    "    total_images = len(images)\n",
    "    cols = (total_images + 1) // 2 \n",
    "    rows = min(2, total_images)\n",
    "    fig = plt.figure(figsize=(7*cols, 15))\n",
    "    \n",
    "    for i, (img, gt_bbox, gt_label) in enumerate(zip(images, gt_boxes, gt_labels)):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(img.permute(1,2,0))\n",
    "            \n",
    "        # Plot ground truth boxes in green\n",
    "        for b, l in zip(gt_bbox, gt_label):\n",
    "            x1, y1, x2, y2 = b\n",
    "            rectangle = patches.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                linewidth=3,\n",
    "                edgecolor='green',\n",
    "                facecolor='none',\n",
    "                label='Ground Truth'\n",
    "            )\n",
    "            ax.add_patch(rectangle)\n",
    "            \n",
    "        # Plot predicted boxes in red if available\n",
    "        if pred_boxes is not None and pred_labels is not None:\n",
    "            pred_bbox = pred_boxes[i]\n",
    "            pred_label = pred_labels[i]\n",
    "            scores = pred_scores[i] if pred_scores is not None else None\n",
    "            \n",
    "            for j, (b, l) in enumerate(zip(pred_bbox, pred_label)):\n",
    "                x1, y1, x2, y2 = b\n",
    "                score_text = f' ({scores[j]:.2f})' if scores is not None else ''\n",
    "                rectangle = patches.Rectangle(\n",
    "                    (x1, y1), x2-x1, y2-y1,\n",
    "                    linewidth=3,\n",
    "                    edgecolor='red',\n",
    "                    facecolor='none',\n",
    "                    label='Prediction'\n",
    "                )\n",
    "                ax.add_patch(rectangle)\n",
    "                # Add score text above the box if available\n",
    "                if scores is not None:\n",
    "                    ax.text(x1, y1-5, f'Score: {scores[j]:.2f}', \n",
    "                           color='red', fontsize=8)\n",
    "        \n",
    "        ax.axis('off')\n",
    "\n",
    "        if legend:\n",
    "            # Add legend \n",
    "            handles = [\n",
    "                patches.Patch(color='green', label='Ground Truth'),\n",
    "                patches.Patch(color='red', label='Prediction')\n",
    "            ]\n",
    "            ax.legend(handles=handles, loc='upper right', prop={'size': 10})\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our dataloading works as expected by visualizing some batches. During training, we use the `albumentations` and `tiatoolbox` library for augmenting the patches. We use a very simple augmentation strategy with some rotations and flips, stain augmenation and defocusing. \n",
    "\n",
    "If you wish to make changes to this augmentation strategy you need to modify the `train_transform` property in `utils/datamodule.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training data loader \n",
    "train_loader = dm.train_dataloader()\n",
    "\n",
    "# Visualize some batches \n",
    "for idx, (images, targets) in enumerate(train_loader):\n",
    "    if idx == 5:\n",
    "        break \n",
    "    \n",
    "    # Extract annotations\n",
    "    boxes = [t['boxes'] for t in targets]\n",
    "    labels = [t['labels'] for t in targets]\n",
    "\n",
    "    # Visulize the images \n",
    "    visualize_images(images, boxes, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the object detection model \n",
    "\n",
    "For the purpose of this notebook we will create a simple FCOS model with a ResNet18 backbone that is trained on a patch size of 512x512. There are other models available in this repository, you can check them out at `utils/model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.factory import ModelFactory \n",
    "\n",
    "# Set up model configurations \n",
    "model = 'FCOS'\n",
    "lr = 0.0001\n",
    "num_classes = 2                 # mitotic figure + background \n",
    "backbone = 'resnet18'\n",
    "weights = 'IMAGENET1K_V1'\n",
    "optimizer = 'AdamW'\n",
    "\n",
    "# init model settings\n",
    "model_kwargs = {\n",
    "    'num_classes': num_classes,\n",
    "    'backbone': backbone,\n",
    "    'weights': weights,\n",
    "    'patch_size': patch_size\n",
    "}\n",
    "\n",
    "# init module settings \n",
    "module_kwargs = {\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': None\n",
    "}\n",
    "\n",
    "fcos = ModelFactory.create(\n",
    "    model_name=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    module_kwargs=module_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up lightning trainer and callbacks\n",
    "\n",
    "The goal of this notebook is to demonstrate some of the functionalities of this repository to get you started. To make use of the full training pipeline, you should use the `train.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "# Init the trainer \n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator='gpu',\n",
    "    logger=False,\n",
    "    gradient_clip_val=1,\n",
    "    reload_dataloaders_every_n_epochs=1\n",
    ")\n",
    "\n",
    "# Start training \n",
    "trainer.fit(fcos, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we can get decent patch-based performance on the validation set. We can easily evaluate the model on some patches of the test in the following. \n",
    "\n",
    "**Note: Here we only perform a patch-based evaluation where the probability of mitotic figures is higher than in the real world setting. Hence, you also need to evaluate your model over the entire images of the test split. This could lead to higher number of false positives. The evaluation over the entire image is outside the scope of this notebook but is included in this repository in the `optimize_threshold.py`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on patch-based evaluation of the test split \n",
    "trainer.test(fcos, dataloaders=dm.test_dataloader(), ckpt_path='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize some of the predictions on cases from the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test dataloader \n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "# Set model to eval mode\n",
    "fcos.eval()\n",
    "fcos.to('cuda')\n",
    "\n",
    "# Perform inference on some test patches \n",
    "for idx, (images, targets) in enumerate(test_loader):\n",
    "\n",
    "    if idx == 10:\n",
    "        break \n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = [img.to('cuda') for img in images]\n",
    "        preds = fcos(images)\n",
    "\n",
    "    # Extract annotations\n",
    "    gt_boxes = [t['boxes'] for t in targets]\n",
    "    gt_labels = [t['labels'] for t in targets]\n",
    "\n",
    "    # Extract predictions\n",
    "    pred_boxes = [p['boxes'].cpu() for p in preds]\n",
    "    pred_labels = [p['labels'].cpu() for p in preds]\n",
    "    pred_scores = [p['scores'].cpu() for p in preds]\n",
    "\n",
    "    # Visulize the images with precictions\n",
    "    visualize_images([img.cpu() for img in images], gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores, legend=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
